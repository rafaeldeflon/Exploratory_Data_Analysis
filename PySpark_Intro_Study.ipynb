{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUfPj97j4ZX3h76bnJetxm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafaeldeflon/Exploratory_Data_Analysis/blob/main/PySpark_Intro_Study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Dataframes\n",
        "For each instruction, type the appropriate code into the cell below the instruction. Then, run the code by pressing the Run button above."
      ],
      "metadata": {
        "id": "aGUvZVW21xO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import the SparkSession class:\n",
        "\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "EXRfVeAA12Jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTM5cf9s37d2",
        "outputId": "ef64de9d-f8c0-4fe7-dda6-dd52b9a867b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=b3a159f33ebcf9133c021347fcbc0e574d2bab0134edb1f2eb7757d320f3cbdd\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "2g4KP3FT2Ztx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Use this class to instiate a Spark session:\n",
        "    \n",
        "    spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"My First PySpark App\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "lgePYW2q2Ezr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession \\\n",
        "  .builder \\\n",
        "  .appName(\"My First PySpark App\") \\\n",
        "  .getOrCreate()"
      ],
      "metadata": {
        "id": "wr4ASnN72aHj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. Take a look at the session object:\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "UMbAvnsg1yJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "pMWXX3bd2kGQ",
        "outputId": "9ab20eb3-919a-4a7d-d1fe-1b1cf3cff28b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7a665bee5930>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7b0e354e4fae:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>My First PySpark App</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Read the contents of a csv file into a Dataframe named 'accounts':\n",
        "\n",
        "accounts = spark.read.option('header', 'true').csv('./data/accounts.csv')"
      ],
      "metadata": {
        "id": "huy3Jvl_2knl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls sample_data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBubD2IP5KHT",
        "outputId": "a647344a-4836-42bf-a837-a1bdc3f81a33"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anscombe.json\t\t     california_housing_train.csv  mnist_train_small.csv\n",
            "california_housing_test.csv  mnist_test.csv\t\t   README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "california_housing = spark.read.option('header', 'true').csv('./sample_data/california_housing_test.csv')"
      ],
      "metadata": {
        "id": "4rhztsry2okN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Take a look at the Dataframe's schema:\n",
        "\n",
        "accounts.printSchema()"
      ],
      "metadata": {
        "id": "fQ0ifuzp2pa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "california_housing.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "betKRmoT2ske",
        "outputId": "883c48c5-8901-4408-b469-78ba6e6e0e55"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- longitude: string (nullable = true)\n",
            " |-- latitude: string (nullable = true)\n",
            " |-- housing_median_age: string (nullable = true)\n",
            " |-- total_rooms: string (nullable = true)\n",
            " |-- total_bedrooms: string (nullable = true)\n",
            " |-- population: string (nullable = true)\n",
            " |-- households: string (nullable = true)\n",
            " |-- median_income: string (nullable = true)\n",
            " |-- median_house_value: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Read the contents of a parquet file into a variable:\n",
        "\n",
        "transactions = spark.read.option('header', True).parquet('./data/transactions.parquet')"
      ],
      "metadata": {
        "id": "S-OTLwfv2tAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "california_housing_train = spark.read.option('header', True).parquet('./sample_data/california_housing_train.parquet')"
      ],
      "metadata": {
        "id": "Lye5NhYr2wME"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. See how many rows are in the new Dataframe:\n",
        "\n",
        "transactions.count()"
      ],
      "metadata": {
        "id": "4gh-zjY92ylc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "california_housing_train.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlZ1V5jU21AK",
        "outputId": "d87a29c2-9c44-4b09-d20e-8571271514ae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17000"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample of the Dataframe california_housing_train\n",
        "california_housing_train.sample(0.1).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfkW32Zm7Sii",
        "outputId": "bd2a0cf4-17fd-4774-895d-a8be54ecb1f9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|  -114.57|   33.57|              20.0|     1454.0|         326.0|     624.0|     262.0|        1.925|           65500.0|\n",
            "|   -114.6|   34.83|              46.0|     1497.0|         309.0|     787.0|     271.0|       2.1908|           48100.0|\n",
            "|  -115.48|   32.68|              15.0|     3414.0|         666.0|    2097.0|     622.0|       2.3319|           91200.0|\n",
            "|  -115.49|   32.87|              19.0|      541.0|         104.0|     457.0|     106.0|       3.3583|          102800.0|\n",
            "|   -115.5|   32.68|              18.0|     3631.0|         913.0|    3565.0|     924.0|       1.5931|           88400.0|\n",
            "|  -115.53|   34.91|              12.0|      807.0|         199.0|     246.0|     102.0|       2.5391|           40000.0|\n",
            "|  -115.53|   32.99|              25.0|     2578.0|         634.0|    2082.0|     565.0|       1.7159|           62200.0|\n",
            "|  -115.55|   32.79|              23.0|     1004.0|         221.0|     697.0|     201.0|       1.6351|           59600.0|\n",
            "|  -115.56|   32.78|              46.0|     2511.0|         490.0|    1583.0|     469.0|       3.0603|           70800.0|\n",
            "|  -115.56|   32.76|              15.0|     1278.0|         217.0|     653.0|     185.0|       4.4821|          140300.0|\n",
            "|  -115.58|   33.88|              21.0|     1161.0|         282.0|     724.0|     186.0|       3.1827|           71700.0|\n",
            "|  -115.58|   32.81|               5.0|      805.0|         143.0|     458.0|     143.0|        4.475|           96300.0|\n",
            "|  -115.58|   32.78|               5.0|     2494.0|         414.0|    1416.0|     421.0|       5.7843|          110100.0|\n",
            "|  -115.73|   33.09|              27.0|      452.0|         103.0|     258.0|      61.0|          2.9|           87500.0|\n",
            "|  -116.01|   33.41|              20.0|     1996.0|         515.0|     659.0|     295.0|       2.8684|           62800.0|\n",
            "|  -116.21|   33.72|              28.0|     2488.0|         714.0|    2891.0|     676.0|       2.3169|           68900.0|\n",
            "|  -116.23|   33.71|              17.0|     4874.0|        1349.0|    5032.0|    1243.0|        2.444|           90000.0|\n",
            "|  -116.24|   33.73|              14.0|     2774.0|         566.0|    1530.0|     505.0|       3.0682|          104100.0|\n",
            "|  -116.32|   33.28|              19.0|     1791.0|         367.0|     327.0|     185.0|       3.3625|          100000.0|\n",
            "|  -116.36|   33.88|              11.0|    12557.0|        3098.0|    2453.0|    1232.0|       1.7844|           78500.0|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new column classifying the rows on the Dataframe california_housing_train by the int part of the column 'latitude'\n",
        "\n",
        "from pyspark.sql.functions import floor\n",
        "\n",
        "california_housing_train = california_housing_train.withColumn('latitude_int', floor(california_housing_train['latitude']))\n"
      ],
      "metadata": {
        "id": "j-Lda7twDPRR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "california_housing_train.sample(0.1).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XGW4dxxDsgM",
        "outputId": "4228cb53-2d18-44d2-bd98-8080bfb3bfea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|latitude_int|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+------------+\n",
            "|  -114.65|   34.89|              17.0|     2556.0|         587.0|    1005.0|     401.0|       1.6991|           69100.0|          34|\n",
            "|  -115.51|   33.12|              21.0|     1024.0|         218.0|     890.0|     232.0|        2.101|           46700.0|          33|\n",
            "|  -115.52|   32.98|              32.0|     1615.0|         382.0|    1307.0|     345.0|       1.4583|           58600.0|          32|\n",
            "|  -115.55|    32.8|              23.0|      666.0|         142.0|     580.0|     160.0|       2.1136|           61000.0|          32|\n",
            "|  -115.55|   32.79|              23.0|     1004.0|         221.0|     697.0|     201.0|       1.6351|           59600.0|          32|\n",
            "|  -115.57|   32.85|              17.0|     1039.0|         256.0|     728.0|     246.0|       1.7411|           63500.0|          32|\n",
            "|  -115.58|   32.79|              14.0|     1687.0|         507.0|     762.0|     451.0|       1.6635|           64400.0|          32|\n",
            "|  -116.01|   33.41|              20.0|     1996.0|         515.0|     659.0|     295.0|       2.8684|           62800.0|          33|\n",
            "|  -116.15|   33.69|              22.0|      197.0|          54.0|     331.0|      70.0|       2.9286|          112500.0|          33|\n",
            "|   -116.2|    33.7|              26.0|     2399.0|         625.0|    2654.0|     535.0|       2.2989|           60600.0|          33|\n",
            "|  -116.33|   34.15|              13.0|     1808.0|         411.0|     735.0|     320.0|       1.5489|           57400.0|          34|\n",
            "|  -116.38|    34.2|              14.0|     4985.0|        1238.0|    2517.0|     954.0|       2.0674|           65000.0|          34|\n",
            "|  -116.38|   33.71|              17.0|    12509.0|        2460.0|    2737.0|    1423.0|       4.5556|          258100.0|          33|\n",
            "|  -116.39|   33.72|              19.0|     7646.0|        1618.0|    2496.0|    1075.0|       3.5569|          128000.0|          33|\n",
            "|  -116.42|   33.51|              26.0|      186.0|          48.0|     102.0|      39.0|       2.5625|          103100.0|          33|\n",
            "|  -116.53|   33.85|              16.0|    10077.0|        2186.0|    3048.0|    1337.0|       2.9647|          110900.0|          33|\n",
            "|  -116.55|   33.84|              28.0|     2992.0|         562.0|     676.0|     346.0|       5.7613|          500001.0|          33|\n",
            "|  -116.62|   32.86|              18.0|     4115.0|         847.0|    2032.0|     745.0|       4.0159|          169100.0|          32|\n",
            "|  -116.68|   33.71|              21.0|     3460.0|         711.0|     658.0|     255.0|       3.5882|          161100.0|          33|\n",
            "|  -116.76|   34.29|              14.0|     3959.0|         849.0|    1064.0|     376.0|       2.8214|          111400.0|          34|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Make a new Dataframe by grouping the transactions by account number and summing the groups. This will combine the transactions per account:\n",
        "\n",
        "account_transactions = transactions.groupby('account_number').sum()"
      ],
      "metadata": {
        "id": "jClKPPA-21Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing_latitude = california_housing_train.groupby('latitude_int').sum('median_income')"
      ],
      "metadata": {
        "id": "RJqjAGuF2385"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Combine the accounts with the summed transaction values:\n",
        "\n",
        "with_sum = accounts.join(account_transactions, 'account_number', 'inner')"
      ],
      "metadata": {
        "id": "_tnb6gWI27sG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oIbDRe3h294S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Get the current balance per account by summing the transaction sums with the initial account balance:\n",
        "\n",
        "accounts = with_sum.withColumn('new_balance', sum([with_sum.balance, with_sum['sum(amount)']]))"
      ],
      "metadata": {
        "id": "vPVq52aj2_HE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ic-2RVeN3BWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Get accounts with negative current balances:\n",
        "\n",
        "neg_balance = accounts.filter(accounts.new_balance < 0)"
      ],
      "metadata": {
        "id": "60vUzZ793C31"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ww56pyCE3FBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Read client data from a json file:\n",
        "\n",
        "clients = spark.read.json('./data/clients.json')"
      ],
      "metadata": {
        "id": "V9uZ_yM-3GCo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "olR3Dq8c3IOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Get the clients with a negative balance:\n",
        "\n",
        "clients = clients.join(neg_balance, 'account_number', 'inner')"
      ],
      "metadata": {
        "id": "UH-QOIaf3JSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lc4vSLDm3Noh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Look at the top five clients with negative balances:\n",
        "\n",
        "clients.select(['first_name', 'last_name', 'account_number', 'new_balance']).show(5)"
      ],
      "metadata": {
        "id": "79CpTBJX3PMW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rP91EZh73RUz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}